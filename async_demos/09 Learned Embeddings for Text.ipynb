{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbvIxBTJyxzv"
   },
   "outputs": [],
   "source": [
    "# Import our standard libraries.\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns  # for nicer plots\n",
    "sns.set(style='darkgrid')  # default style\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXq4Ys5kLM9A"
   },
   "source": [
    "## Understanding the Embeddings Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZILRy2bDM81P"
   },
   "outputs": [],
   "source": [
    "# Input shape:  (batch_size, input_length)\n",
    "# Output shape: (batch_size, input_length, output_dim)\n",
    "embeddings = tf.keras.layers.Embedding(\n",
    "    input_dim = 100,  # size of feature vocabulary\n",
    "    output_dim = 2,   # embedding dimension\n",
    "    input_length = 5  # number of inputs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ftNkJwzN_Xu"
   },
   "outputs": [],
   "source": [
    "# Get embeddings for the input ids [0, 1, 2, 3, 4]\n",
    "data = tf.constant([0, 1, 2, 3, 4], shape=(1, 5))\n",
    "embed_data = embeddings(data)\n",
    "embed_data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHajItNaSD61"
   },
   "outputs": [],
   "source": [
    "# Average embeddings\n",
    "embed_data_average = tf.keras.layers.GlobalAveragePooling1D()(embed_data)\n",
    "embed_data_average.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uS1vgfpI3k89"
   },
   "source": [
    "## Embeddings for Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PGO-lK3J0R-"
   },
   "source": [
    "Let's store our small set of movie reviews and their labels in numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8rFCBLrV4AQ3"
   },
   "outputs": [],
   "source": [
    "X_train = np.array([\n",
    "                    'This movie was amazing',\n",
    "                    'I have seen it 8 times !',\n",
    "                    'I fell asleep',\n",
    "                    'I would not recommend it',\n",
    "                    'It was absolutely awful',\n",
    "                    'I would watch it again !'\n",
    "                  ])\n",
    "\n",
    "Y_train = np.array([\n",
    "                    1,\n",
    "                    1,\n",
    "                    0,\n",
    "                    0,  \n",
    "                    0, \n",
    "                    1\n",
    "                  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__eF8JYS5WK5"
   },
   "outputs": [],
   "source": [
    "display(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JGHiApe5fiY"
   },
   "outputs": [],
   "source": [
    "max_sequence_length = 6\n",
    "\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=None, # Maximum vocabulary size - None -> no cap\n",
    "    standardize='lower_and_strip_punctuation', # Standarization to apply to text - None -> no standarization\n",
    "    split=\"whitespace\", # Values can be None (no splitting), \"whitespace\", or a Callable\n",
    "    output_mode='int',  # Values can be \"int\", \"multi_hot\", \"count\" or \"tf_idf\"\n",
    "    output_sequence_length=max_sequence_length, # Only valid in INT mode. If set, the output will have its time dimension padded or truncated to exactly output_sequence_length values\n",
    "    )\n",
    "\n",
    "vectorize_layer.adapt(X_train)\n",
    "\n",
    "display(\"--Vocabulary--\")\n",
    "for i, token in enumerate(vectorize_layer.get_vocabulary()):\n",
    "  display('%d: %s' %(i, token))\n",
    "\n",
    "# 0: ('') - Padding Token\n",
    "# 1: ('[UNK]') - OOV Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F1iNTJ7v-DfY"
   },
   "outputs": [],
   "source": [
    "X_train_vectorized = vectorize_layer(X_train)\n",
    "\n",
    "display(X_train_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MjqFXONSCtJI"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vectorize_layer.get_vocabulary())\n",
    "\n",
    "# Input shape:  (batch_size, input_length)\n",
    "# Output shape: (batch_size, input_length, output_dim)\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim = vocab_size,  # size of feature vocabulary\n",
    "    output_dim = 3,   # embedding dimension\n",
    "    input_length = max_sequence_length  # number of inputs\n",
    "    )\n",
    "\n",
    "first_review_embed_rep = embedding_layer(X_train_vectorized[0])\n",
    "display(first_review_embed_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jXiFgOojy5m-"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "  tf.keras.backend.clear_session()\n",
    "  tf.random.set_seed(0)\n",
    "\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(vectorize_layer)\n",
    "  model.add(tf.keras.layers.Embedding(\n",
    "      input_dim = vocab_size,  # size of feature vocabulary\n",
    "      output_dim = 2,  # embedding dimension\n",
    "      input_length = max_sequence_length  # number of inputs\n",
    "      ))\n",
    "\n",
    "  # Average over the sequence dimension, so each review is represented by \n",
    "  # 1 vector of size embedding_dimension\n",
    "  model.add(tf.keras.layers.GlobalAveragePooling1D()) \n",
    "\n",
    "  # Alternatively, we could concatenate the embedding representations of \n",
    "  # all tokens in the movie review\n",
    "  #model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(\n",
    "      units=8,        \n",
    "      activation='relu'))\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(\n",
    "      units=1,        \n",
    "      activation='sigmoid'))\n",
    "\n",
    "  model.compile(loss='binary_crossentropy', \n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38BFoq44G0bG"
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "\n",
    "# Display the model layers.\n",
    "display(model.layers)\n",
    "display(model.summary())\n",
    "\n",
    "# Retrieve the embeddings layer, which itself is wrapped in a list.\n",
    "embeddings = model.layers[1].get_weights()[0]\n",
    "display(\"Embeddings layer - shape: \", embeddings.shape)\n",
    "display(\"Embeddings layer - parameter matrix (before training): \", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MebyIPtzOApO"
   },
   "outputs": [],
   "source": [
    "def plot_embeddings(model):\n",
    "  embeddings = model.layers[1].get_weights()[0]\n",
    "  plt.scatter(embeddings[:,0], embeddings[:,1])\n",
    "  for i, token in enumerate(vectorize_layer.get_vocabulary()):\n",
    "    plt.annotate(token, (embeddings[i]))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWVOJsbyJ53A"
   },
   "outputs": [],
   "source": [
    "plot_embeddings(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_rmbLCqU_72d"
   },
   "outputs": [],
   "source": [
    "display(X_train)\n",
    "display(model.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOheiJzMIGrS"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "  x = X_train,  # our sparse padded training data\n",
    "  y = Y_train,  # corresponding binary labels\n",
    "  epochs=15,    # number of passes through the training data\n",
    "  verbose=0     # display some progress output during training\n",
    "  )\n",
    "\n",
    "plot_embeddings(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MycKjnpTK5nh"
   },
   "outputs": [],
   "source": [
    "display(X_train)\n",
    "display(model.predict(X_train))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPdgKagw6Y9Ql3IUcl6ZRZM",
   "collapsed_sections": [],
   "name": "Learned Embeddings for Text.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
